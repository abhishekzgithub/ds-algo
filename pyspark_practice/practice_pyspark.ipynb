{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "522e865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "spark=SparkSession.builder.appName(\"jupyter\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb9da2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "filepath=r\"C:\\Users\\abhishek\\Documents\\excel_work\\stratascratch.xls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64f13457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=spark.sparkContext.parallelize(range(23,100))#.count()\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "31220a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flag_id</th>\n",
       "      <th>reviewed_by_yt</th>\n",
       "      <th>reviewed_date</th>\n",
       "      <th>reviewed_outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0cazx3</td>\n",
       "      <td>False</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1cn76u</td>\n",
       "      <td>True</td>\n",
       "      <td>2022-03-15</td>\n",
       "      <td>REMOVED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1i43zk</td>\n",
       "      <td>True</td>\n",
       "      <td>2022-03-15</td>\n",
       "      <td>REMOVED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1n0vef</td>\n",
       "      <td>True</td>\n",
       "      <td>2022-03-15</td>\n",
       "      <td>REMOVED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1sv6ib</td>\n",
       "      <td>True</td>\n",
       "      <td>2022-03-15</td>\n",
       "      <td>APPROVED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  flag_id  reviewed_by_yt reviewed_date reviewed_outcome\n",
       "0  0cazx3           False           NaT              NaN\n",
       "1  1cn76u            True    2022-03-15          REMOVED\n",
       "2  1i43zk            True    2022-03-15          REMOVED\n",
       "3  1n0vef            True    2022-03-15          REMOVED\n",
       "4  1sv6ib            True    2022-03-15         APPROVED"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sheet=\"flag_review\"\n",
    "df2=pd.read_excel(filepath,sheet_name=sheet)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7c1bc3f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------------+----------------+\n",
      "|flag_id|reviewed_by_yt|      reviewed_date|reviewed_outcome|\n",
      "+-------+--------------+-------------------+----------------+\n",
      "| 0cazx3|         false|               null|             NaN|\n",
      "| 1cn76u|          true|2022-03-15 00:00:00|         REMOVED|\n",
      "| 1i43zk|          true|2022-03-15 00:00:00|         REMOVED|\n",
      "| 1n0vef|          true|2022-03-15 00:00:00|         REMOVED|\n",
      "| 1sv6ib|          true|2022-03-15 00:00:00|        APPROVED|\n",
      "| 20xekb|          true|2022-03-17 00:00:00|         REMOVED|\n",
      "| 4cvwuv|          true|2022-03-15 00:00:00|        APPROVED|\n",
      "| 4l1tk7|         false|               null|             NaN|\n",
      "| 4sd6dv|          true|2022-03-14 00:00:00|         REMOVED|\n",
      "| 6jjkvn|          true|2022-03-16 00:00:00|        APPROVED|\n",
      "| 7ks264|          true|2022-03-15 00:00:00|        APPROVED|\n",
      "| 8946nx|         false|               null|             NaN|\n",
      "| 8wwg0l|         false|               null|             NaN|\n",
      "| arydfd|          true|2022-03-15 00:00:00|        APPROVED|\n",
      "| bl40qw|          true|2022-03-16 00:00:00|         REMOVED|\n",
      "| ehn1pt|          true|2022-03-18 00:00:00|        APPROVED|\n",
      "| hucyzx|         false|               null|             NaN|\n",
      "| i2l3oo|          true|2022-03-17 00:00:00|         REMOVED|\n",
      "| i6336w|         false|               null|             NaN|\n",
      "| iey5vi|         false|               null|             NaN|\n",
      "+-------+--------------+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.pandas import read_excel\n",
    "df=spark.createDataFrame(df2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f124ed6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"reviewed_by_yt\").count().orderBy('count',descending=False).limit(2).select(\"reviewed_by_yt\").rdd.flatMap(lambda x:x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "990b0a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------------+----------------+\n",
      "|flag_id|reviewed_by_yt|      reviewed_date|reviewed_outcome|\n",
      "+-------+--------------+-------------------+----------------+\n",
      "| 0cazx3|         false|               null|             NaN|\n",
      "| 1cn76u|          true|2022-03-15 00:00:00|         REMOVED|\n",
      "| 1i43zk|          true|2022-03-15 00:00:00|         REMOVED|\n",
      "| 1n0vef|          true|2022-03-15 00:00:00|         REMOVED|\n",
      "| 1sv6ib|          true|2022-03-15 00:00:00|        APPROVED|\n",
      "| 20xekb|          true|2022-03-17 00:00:00|         REMOVED|\n",
      "| 4cvwuv|          true|2022-03-15 00:00:00|        APPROVED|\n",
      "| 4l1tk7|         false|               null|             NaN|\n",
      "| 4sd6dv|          true|2022-03-14 00:00:00|         REMOVED|\n",
      "| 6jjkvn|          true|2022-03-16 00:00:00|        APPROVED|\n",
      "| 7ks264|          true|2022-03-15 00:00:00|        APPROVED|\n",
      "| 8946nx|         false|               null|             NaN|\n",
      "| 8wwg0l|         false|               null|             NaN|\n",
      "| arydfd|          true|2022-03-15 00:00:00|        APPROVED|\n",
      "| bl40qw|          true|2022-03-16 00:00:00|         REMOVED|\n",
      "| ehn1pt|          true|2022-03-18 00:00:00|        APPROVED|\n",
      "| hucyzx|         false|               null|             NaN|\n",
      "| i2l3oo|          true|2022-03-17 00:00:00|         REMOVED|\n",
      "| i6336w|         false|               null|             NaN|\n",
      "| iey5vi|         false|               null|             NaN|\n",
      "+-------+--------------+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.dropna(how=\"any\",subset=[\"reviewed_outcome\"]).show()#,col=\"reviewed_outcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75c39b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------------+----------------+\n",
      "|flag_id|reviewed_by_yt|      reviewed_date|reviewed_outcome|\n",
      "+-------+--------------+-------------------+----------------+\n",
      "| 1cn76u|          true|2022-03-15 00:00:00|         REMOVED|\n",
      "| 1i43zk|          true|2022-03-15 00:00:00|         REMOVED|\n",
      "| 1n0vef|          true|2022-03-15 00:00:00|         REMOVED|\n",
      "| 1sv6ib|          true|2022-03-15 00:00:00|        APPROVED|\n",
      "| 20xekb|          true|2022-03-17 00:00:00|         REMOVED|\n",
      "| 4cvwuv|          true|2022-03-15 00:00:00|        APPROVED|\n",
      "| 4sd6dv|          true|2022-03-14 00:00:00|         REMOVED|\n",
      "| 6jjkvn|          true|2022-03-16 00:00:00|        APPROVED|\n",
      "| 7ks264|          true|2022-03-15 00:00:00|        APPROVED|\n",
      "| arydfd|          true|2022-03-15 00:00:00|        APPROVED|\n",
      "| bl40qw|          true|2022-03-16 00:00:00|         REMOVED|\n",
      "| ehn1pt|          true|2022-03-18 00:00:00|        APPROVED|\n",
      "| i2l3oo|          true|2022-03-17 00:00:00|         REMOVED|\n",
      "| kc41jd|          true|2022-03-14 00:00:00|         REMOVED|\n",
      "| kktiwe|          true|2022-03-14 00:00:00|        APPROVED|\n",
      "| ov5gd8|          true|2022-03-17 00:00:00|        APPROVED|\n",
      "| xciyse|          true|2022-03-16 00:00:00|        APPROVED|\n",
      "| xvhk6d|          true|2022-03-17 00:00:00|        APPROVED|\n",
      "+-------+--------------+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.reviewed_outcome!='NaN').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f326a3ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|reviewed_by_yt|reviewed_by_yt_upper|\n",
      "+--------------+--------------------+\n",
      "|         false|                   0|\n",
      "|          true|                   1|\n",
      "|          true|                   1|\n",
      "|          true|                   1|\n",
      "|          true|                   1|\n",
      "|          true|                   1|\n",
      "|          true|                   1|\n",
      "|         false|                   0|\n",
      "|          true|                   1|\n",
      "|          true|                   1|\n",
      "|          true|                   1|\n",
      "|         false|                   0|\n",
      "|         false|                   0|\n",
      "|          true|                   1|\n",
      "|          true|                   1|\n",
      "|          true|                   1|\n",
      "|         false|                   0|\n",
      "|          true|                   1|\n",
      "|         false|                   0|\n",
      "|         false|                   0|\n",
      "+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "@udf(returnType=StringType()) \n",
    "def capitaliseUDF(row):\n",
    "    #if isinstance(row,str):\n",
    "    if row:\n",
    "        row=1\n",
    "    else:\n",
    "        row=0\n",
    "    return row\n",
    "#capitaliseUDF=udf(lambda z: capitalise(z))\n",
    "df.select(\"reviewed_by_yt\",capitaliseUDF(\"reviewed_by_yt\").alias(\"reviewed_by_yt_upper\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a104b078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------------+----------------+--------------------+\n",
      "|flag_id|reviewed_by_yt|      reviewed_date|reviewed_outcome|reviewed_by_yt_upper|\n",
      "+-------+--------------+-------------------+----------------+--------------------+\n",
      "| 0cazx3|         false|               null|             NaN|                   0|\n",
      "| 1cn76u|          true|2022-03-15 00:00:00|         REMOVED|                   1|\n",
      "| 1i43zk|          true|2022-03-15 00:00:00|         REMOVED|                   1|\n",
      "| 1n0vef|          true|2022-03-15 00:00:00|         REMOVED|                   1|\n",
      "| 1sv6ib|          true|2022-03-15 00:00:00|        APPROVED|                   1|\n",
      "| 20xekb|          true|2022-03-17 00:00:00|         REMOVED|                   1|\n",
      "| 4cvwuv|          true|2022-03-15 00:00:00|        APPROVED|                   1|\n",
      "| 4l1tk7|         false|               null|             NaN|                   0|\n",
      "| 4sd6dv|          true|2022-03-14 00:00:00|         REMOVED|                   1|\n",
      "| 6jjkvn|          true|2022-03-16 00:00:00|        APPROVED|                   1|\n",
      "| 7ks264|          true|2022-03-15 00:00:00|        APPROVED|                   1|\n",
      "| 8946nx|         false|               null|             NaN|                   0|\n",
      "| 8wwg0l|         false|               null|             NaN|                   0|\n",
      "| arydfd|          true|2022-03-15 00:00:00|        APPROVED|                   1|\n",
      "| bl40qw|          true|2022-03-16 00:00:00|         REMOVED|                   1|\n",
      "| ehn1pt|          true|2022-03-18 00:00:00|        APPROVED|                   1|\n",
      "| hucyzx|         false|               null|             NaN|                   0|\n",
      "| i2l3oo|          true|2022-03-17 00:00:00|         REMOVED|                   1|\n",
      "| i6336w|         false|               null|             NaN|                   0|\n",
      "| iey5vi|         false|               null|             NaN|                   0|\n",
      "+-------+--------------+-------------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"reviewed_by_yt_upper\",capitaliseUDF(\"reviewed_by_yt\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c1e548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------------+----------------+-----------------+\n",
      "|flag_id|reviewed_by_yt|      reviewed_date|reviewed_outcome|flag_id_wordcount|\n",
      "+-------+--------------+-------------------+----------------+-----------------+\n",
      "| 0cazx3|         false|               null|             NaN|                6|\n",
      "| 1cn76u|          true|2022-03-15 00:00:00|         REMOVED|                6|\n",
      "| 1i43zk|          true|2022-03-15 00:00:00|         REMOVED|                6|\n",
      "| 1n0vef|          true|2022-03-15 00:00:00|         REMOVED|                6|\n",
      "| 1sv6ib|          true|2022-03-15 00:00:00|        APPROVED|                6|\n",
      "| 20xekb|          true|2022-03-17 00:00:00|         REMOVED|                6|\n",
      "| 4cvwuv|          true|2022-03-15 00:00:00|        APPROVED|                6|\n",
      "| 4l1tk7|         false|               null|             NaN|                6|\n",
      "| 4sd6dv|          true|2022-03-14 00:00:00|         REMOVED|                6|\n",
      "| 6jjkvn|          true|2022-03-16 00:00:00|        APPROVED|                6|\n",
      "| 7ks264|          true|2022-03-15 00:00:00|        APPROVED|                6|\n",
      "| 8946nx|         false|               null|             NaN|                6|\n",
      "| 8wwg0l|         false|               null|             NaN|                6|\n",
      "| arydfd|          true|2022-03-15 00:00:00|        APPROVED|                6|\n",
      "| bl40qw|          true|2022-03-16 00:00:00|         REMOVED|                6|\n",
      "| ehn1pt|          true|2022-03-18 00:00:00|        APPROVED|                6|\n",
      "| hucyzx|         false|               null|             NaN|                6|\n",
      "| i2l3oo|          true|2022-03-17 00:00:00|         REMOVED|                6|\n",
      "| i6336w|         false|               null|             NaN|                6|\n",
      "| iey5vi|         false|               null|             NaN|                6|\n",
      "+-------+--------------+-------------------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udf\n",
    "def count_characters(row):\n",
    "    if row:\n",
    "        return len(row)\n",
    "\n",
    "df.withColumn(\"flag_id_wordcount\",count_characters(\"flag_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42bce484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   name|age|salary|\n",
      "+-------+---+------+\n",
      "|  James| 34| 55000|\n",
      "|Michael| 30| 70000|\n",
      "| Robert| 37| 60000|\n",
      "|  Maria| 29| 80000|\n",
      "|    Jen| 32| 65000|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "\n",
    "df_difference = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "df_difference.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2cc91a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+----------------+\n",
      "|   name|age|salary|lag_salary|differece_salary|\n",
      "+-------+---+------+----------+----------------+\n",
      "|  James| 34| 55000|      null|            null|\n",
      "| Robert| 37| 60000|     55000|            5000|\n",
      "|    Jen| 32| 65000|     60000|            5000|\n",
      "|Michael| 30| 70000|     65000|            5000|\n",
      "|  Maria| 29| 80000|     70000|           10000|\n",
      "+-------+---+------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lag, desc,asc\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window=Window().orderBy(asc(col(\"salary\")))#.descending\n",
    "#.partitionBy(\"age\").orderBy(\"name\")\n",
    "\n",
    "df_difference1=df_difference.withColumn(\"lag_salary\",lag(df_difference.salary).over(window))\n",
    "df_difference1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f21b0af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+----------+----------------+\n",
      "|   name|age|salary|lag_salary|differece_salary|\n",
      "+-------+---+------+----------+----------------+\n",
      "|  James| 34| 55000|      null|            null|\n",
      "| Robert| 37| 60000|     55000|            5000|\n",
      "|    Jen| 32| 65000|     60000|            5000|\n",
      "|Michael| 30| 70000|     65000|            5000|\n",
      "|  Maria| 29| 80000|     70000|           10000|\n",
      "+-------+---+------+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_difference=df_difference1.withColumn(\"differece_salary\",df_difference1.salary-df_difference1.lag_salary)\n",
    "df_difference.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7606c323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|      date|random_numbers|\n",
      "+----------+--------------+\n",
      "|2000-01-01|             8|\n",
      "|2000-01-08|             3|\n",
      "|2000-01-15|             8|\n",
      "|2000-01-22|             5|\n",
      "|2000-01-29|             4|\n",
      "|2000-02-05|             6|\n",
      "|2000-02-12|             8|\n",
      "|2000-02-19|             1|\n",
      "|2000-02-26|             9|\n",
      "|2000-03-04|             3|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, explode, sequence, rand\n",
    "\n",
    "# Start date and end date (start + 10 weekends)\n",
    "start_date = '2000-01-01'\n",
    "end_date = '2000-03-04' # Calculated manually: 10 weekends (Saturdays) from start date\n",
    "\n",
    "# Create a DataFrame with one row containing a sequence from start_date to end_date with a 1 day step\n",
    "df = spark.range(1).select(\n",
    "explode(\n",
    "sequence(\n",
    "expr(f\"date '{start_date}'\"),\n",
    "expr(f\"date '{end_date}'\"),\n",
    "expr(\"interval 1 day\")\n",
    ")\n",
    ").alias(\"date\")\n",
    ")\n",
    "\n",
    "# Filter out the weekdays (retain weekends)\n",
    "df = df.filter(expr(\"dayofweek(date) = 7\")) # 7 corresponds to Saturday in Spark\n",
    "\n",
    "# Add the random numbers column\n",
    "#df = df.withColumn(\"random_numbers\", rand()*10)\n",
    "df = df.withColumn(\"random_numbers\", ((rand(seed=42) * 10) + 1).cast(\"int\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d242f4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|       col|\n",
      "+----------+\n",
      "|2000-01-01|\n",
      "|2000-01-02|\n",
      "|2000-01-03|\n",
      "|2000-01-04|\n",
      "|2000-01-05|\n",
      "|2000-01-06|\n",
      "|2000-01-07|\n",
      "|2000-01-08|\n",
      "|2000-01-09|\n",
      "|2000-01-10|\n",
      "|2000-01-11|\n",
      "|2000-01-12|\n",
      "|2000-01-13|\n",
      "|2000-01-14|\n",
      "|2000-01-15|\n",
      "|2000-01-16|\n",
      "|2000-01-17|\n",
      "|2000-01-18|\n",
      "|2000-01-19|\n",
      "|2000-01-20|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.range(1).select(\n",
    "    explode(\n",
    "        sequence(\n",
    "            expr(f\"date '{start_date}'\"),\n",
    "            expr(f\"date '{end_date}'\"),\n",
    "            expr(\"interval 1 day\"))))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c9cb8cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------------+----------------+\n",
      "|flag_id|reviewed_by_yt|      reviewed_date|reviewed_outcome|\n",
      "+-------+--------------+-------------------+----------------+\n",
      "| 0cazx3|         false|               null|             NaN|\n",
      "| 1cn76u|          true|2022-03-15 00:00:00|         REMOVED|\n",
      "| 1i43zk|          true|2022-03-15 00:00:00|         REMOVED|\n",
      "| 1n0vef|          true|2022-03-15 00:00:00|         REMOVED|\n",
      "| 1sv6ib|          true|2022-03-15 00:00:00|        APPROVED|\n",
      "| 20xekb|          true|2022-03-17 00:00:00|         REMOVED|\n",
      "| 4cvwuv|          true|2022-03-15 00:00:00|        APPROVED|\n",
      "| 4l1tk7|         false|               null|             NaN|\n",
      "| 4sd6dv|          true|2022-03-14 00:00:00|         REMOVED|\n",
      "| 6jjkvn|          true|2022-03-16 00:00:00|        APPROVED|\n",
      "| 7ks264|          true|2022-03-15 00:00:00|        APPROVED|\n",
      "| 8946nx|         false|               null|             NaN|\n",
      "| 8wwg0l|         false|               null|             NaN|\n",
      "| arydfd|          true|2022-03-15 00:00:00|        APPROVED|\n",
      "| bl40qw|          true|2022-03-16 00:00:00|         REMOVED|\n",
      "| ehn1pt|          true|2022-03-18 00:00:00|        APPROVED|\n",
      "| hucyzx|         false|               null|             NaN|\n",
      "| i2l3oo|          true|2022-03-17 00:00:00|         REMOVED|\n",
      "| i6336w|         false|               null|             NaN|\n",
      "| iey5vi|         false|               null|             NaN|\n",
      "+-------+--------------+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8e9e5aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "59ba5129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|null_count|\n",
      "+----------+\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         1|\n",
      "|         0|\n",
      "|         1|\n",
      "|         0|\n",
      "|         0|\n",
      "|         1|\n",
      "|         1|\n",
      "|         0|\n",
      "|         0|\n",
      "|         1|\n",
      "|         0|\n",
      "|         1|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "|         0|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    F.when(\n",
    "        F.isnan(df.reviewed_outcome)\n",
    "        |df.reviewed_outcome.contains(\"REMOVED\")\n",
    "        |df.reviewed_outcome.contains(\"None\")\n",
    "        |df.reviewed_outcome.isNull()\n",
    "        , 0)\n",
    "    .otherwise(1).alias(\"null_count\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "244fc2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|Give a timeseries data and userid|\n",
      "+---------------------------------+\n",
      "|             . find the sessio...|\n",
      "|             . the user is log...|\n",
      "|             Find the user act...|\n",
      "|             Find how much tim...|\n",
      "|             timestamp        ...|\n",
      "|             2023-01-01 1:05:0...|\n",
      "|             2023-01-01 2:05:0...|\n",
      "|             2023-01-12 1:05:0...|\n",
      "|             2023-01-01 1:05:0...|\n",
      "|             2023-01-12 1:35:0...|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath=r\"C:\\Users\\abhishek\\Documents\\coding\\ds-algo\\interview_test\\publicis_sapient.txt\"\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(filepath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "68e4b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"userid\", StringType(), True)\n",
    "])\n",
    "schema=\"\"\"\n",
    "timestamp string,\n",
    "userid string\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6f2692c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|           timestamp|userid|\n",
      "+--------------------+------+\n",
      "|2023-01-01 1:05:0...|  null|\n",
      "|2023-01-01 2:05:0...|  null|\n",
      "|2023-01-12 1:05:0...|  null|\n",
      "|2023-01-01 1:05:0...|  null|\n",
      "|2023-01-12 1:35:0...|  null|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#schema=\n",
    "df = spark.read.csv(filepath, schema=schema, sep=\"\\t\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f4454259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|split(timestamp,   , -1)|\n",
      "+------------------------+\n",
      "|    [2023-01-01 1:05:...|\n",
      "|    [2023-01-01 2:05:...|\n",
      "|    [2023-01-12 1:05:...|\n",
      "|    [2023-01-01 1:05:...|\n",
      "|    [2023-01-12 1:35:...|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.split(F.col(\"timestamp\"),\"  \")).show()#\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "9433ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark.sparkContext.textFile(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "295112e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2023-01-01', '1:05:00', '', '', '', '', '', 'u1'],\n",
       " ['2023-01-01', '1:25:00', '', '', '', '', '', 'u1'],\n",
       " ['2023-01-02', '2:05:00', '', '', '', '', '', 'u1'],\n",
       " ['2023-01-02', '2:35:00', '', '', '', '', '', 'u1'],\n",
       " ['2023-01-02', '3:05:00', '', '', '', '', '', 'u1'],\n",
       " ['2023-01-12', '1:05:00', '', '', '', '', '', 'u2'],\n",
       " ['2023-01-12', '1:05:00', '', '', '', '', '', 'u2'],\n",
       " ['2023-01-12', '1:05:00', '', '', '', '', '', 'u2'],\n",
       " ['2023-01-01', '1:05:00', '', '', '', '', '', 'u1'],\n",
       " ['2023-01-12', '1:35:00', '', '', '', '', '', 'u2']]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val=sc.map(lambda row: row.split(\" \")).collect()\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "26d0856f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---+---+---+---+---+---+\n",
      "|        _1|     _2| _3| _4| _5| _6| _7| _8|\n",
      "+----------+-------+---+---+---+---+---+---+\n",
      "|2023-01-01|1:05:00|   |   |   |   |   | u1|\n",
      "|2023-01-01|1:25:00|   |   |   |   |   | u1|\n",
      "|2023-01-02|2:05:00|   |   |   |   |   | u1|\n",
      "|2023-01-02|2:35:00|   |   |   |   |   | u1|\n",
      "|2023-01-02|3:05:00|   |   |   |   |   | u1|\n",
      "|2023-01-12|1:05:00|   |   |   |   |   | u2|\n",
      "|2023-01-12|1:05:00|   |   |   |   |   | u2|\n",
      "|2023-01-12|1:05:00|   |   |   |   |   | u2|\n",
      "|2023-01-01|1:05:00|   |   |   |   |   | u1|\n",
      "|2023-01-12|1:35:00|   |   |   |   |   | u2|\n",
      "+----------+-------+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date=spark.createDataFrame(val)\n",
    "df_date.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ee8c70d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+\n",
      "|      date|   time|userid|\n",
      "+----------+-------+------+\n",
      "|2023-01-01|1:05:00|    u1|\n",
      "|2023-01-01|1:25:00|    u1|\n",
      "|2023-01-02|2:05:00|    u1|\n",
      "|2023-01-02|2:35:00|    u1|\n",
      "|2023-01-02|3:05:00|    u1|\n",
      "|2023-01-12|1:05:00|    u2|\n",
      "|2023-01-12|1:05:00|    u2|\n",
      "|2023-01-12|1:05:00|    u2|\n",
      "|2023-01-01|1:05:00|    u1|\n",
      "|2023-01-12|1:35:00|    u2|\n",
      "+----------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date_formatted=df_date.select(\"_1\",\"_2\",\"_8\").withColumnRenamed(\"_1\",\"date\").withColumnRenamed(\"_2\",\"time\").withColumnRenamed(\"_8\",\"userid\")\n",
    "df_date_formatted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "4d311e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+------------------+\n",
      "|      date|   time|userid|         datetime_|\n",
      "+----------+-------+------+------------------+\n",
      "|2023-01-01|1:05:00|    u1|2023-01-01 1:05:00|\n",
      "|2023-01-01|1:25:00|    u1|2023-01-01 1:25:00|\n",
      "|2023-01-02|2:05:00|    u1|2023-01-02 2:05:00|\n",
      "|2023-01-02|2:35:00|    u1|2023-01-02 2:35:00|\n",
      "|2023-01-02|3:05:00|    u1|2023-01-02 3:05:00|\n",
      "|2023-01-12|1:05:00|    u2|2023-01-12 1:05:00|\n",
      "|2023-01-12|1:05:00|    u2|2023-01-12 1:05:00|\n",
      "|2023-01-12|1:05:00|    u2|2023-01-12 1:05:00|\n",
      "|2023-01-01|1:05:00|    u1|2023-01-01 1:05:00|\n",
      "|2023-01-12|1:35:00|    u2|2023-01-12 1:35:00|\n",
      "+----------+-------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date_formatted=df_date_formatted.withColumn(\"datetime_\",F.concat_ws(\" \",df_date_formatted.date,df_date_formatted.time))\n",
    "df_date_formatted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "ad248ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+------------------+------------------+\n",
      "|      date|   time|userid|         datetime_|       lag_session|\n",
      "+----------+-------+------+------------------+------------------+\n",
      "|2023-01-01|1:05:00|    u1|2023-01-01 1:05:00|              null|\n",
      "|2023-01-01|1:05:00|    u1|2023-01-01 1:05:00|2023-01-01 1:05:00|\n",
      "|2023-01-01|1:25:00|    u1|2023-01-01 1:25:00|2023-01-01 1:05:00|\n",
      "|2023-01-02|2:05:00|    u1|2023-01-02 2:05:00|2023-01-01 1:25:00|\n",
      "|2023-01-02|2:35:00|    u1|2023-01-02 2:35:00|2023-01-02 2:05:00|\n",
      "|2023-01-02|3:05:00|    u1|2023-01-02 3:05:00|2023-01-02 2:35:00|\n",
      "|2023-01-12|1:05:00|    u2|2023-01-12 1:05:00|              null|\n",
      "|2023-01-12|1:05:00|    u2|2023-01-12 1:05:00|2023-01-12 1:05:00|\n",
      "|2023-01-12|1:05:00|    u2|2023-01-12 1:05:00|2023-01-12 1:05:00|\n",
      "|2023-01-12|1:35:00|    u2|2023-01-12 1:35:00|2023-01-12 1:05:00|\n",
      "+----------+-------+------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window=Window().partitionBy(\"userid\").orderBy(F.asc(col(\"datetime_\")))\n",
    "\n",
    "df_date_formatted=df_date_formatted.withColumn(\"lag_session\",F.lag(\"datetime_\").over(window))\n",
    "df_date_formatted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "afab2468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+------------------+------------------+--------------------+\n",
      "|      date|   time|userid|         datetime_|       lag_session|  session_difference|\n",
      "+----------+-------+------+------------------+------------------+--------------------+\n",
      "|2023-01-01|1:05:00|    u1|2023-01-01 1:05:00|              null|                null|\n",
      "|2023-01-01|1:05:00|    u1|2023-01-01 1:05:00|2023-01-01 1:05:00|INTERVAL '0 00:00...|\n",
      "|2023-01-01|1:25:00|    u1|2023-01-01 1:25:00|2023-01-01 1:05:00|INTERVAL '0 00:20...|\n",
      "|2023-01-02|2:05:00|    u1|2023-01-02 2:05:00|2023-01-01 1:25:00|INTERVAL '1 00:40...|\n",
      "|2023-01-02|2:35:00|    u1|2023-01-02 2:35:00|2023-01-02 2:05:00|INTERVAL '0 00:30...|\n",
      "|2023-01-02|3:05:00|    u1|2023-01-02 3:05:00|2023-01-02 2:35:00|INTERVAL '0 00:30...|\n",
      "|2023-01-12|1:05:00|    u2|2023-01-12 1:05:00|              null|                null|\n",
      "|2023-01-12|1:05:00|    u2|2023-01-12 1:05:00|2023-01-12 1:05:00|INTERVAL '0 00:00...|\n",
      "|2023-01-12|1:05:00|    u2|2023-01-12 1:05:00|2023-01-12 1:05:00|INTERVAL '0 00:00...|\n",
      "|2023-01-12|1:35:00|    u2|2023-01-12 1:35:00|2023-01-12 1:05:00|INTERVAL '0 00:30...|\n",
      "+----------+-------+------+------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date_formatted=df_date_formatted.withColumn(\"session_difference\",\\\n",
    "                             F.to_timestamp(df_date_formatted.datetime_)\\\n",
    "                             -F.to_timestamp(df_date_formatted.lag_session))\n",
    "df_date_formatted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "1f9452bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+------------------+------------------+-----------------------------------+\n",
      "|date      |time   |userid|datetime_         |lag_session       |session_difference                 |\n",
      "+----------+-------+------+------------------+------------------+-----------------------------------+\n",
      "|2023-01-01|1:05:00|u1    |2023-01-01 1:05:00|null              |null                               |\n",
      "|2023-01-01|1:05:00|u1    |2023-01-01 1:05:00|2023-01-01 1:05:00|INTERVAL '0 00:00:00' DAY TO SECOND|\n",
      "|2023-01-01|1:25:00|u1    |2023-01-01 1:25:00|2023-01-01 1:05:00|INTERVAL '0 00:20:00' DAY TO SECOND|\n",
      "|2023-01-02|2:05:00|u1    |2023-01-02 2:05:00|2023-01-01 1:25:00|INTERVAL '1 00:40:00' DAY TO SECOND|\n",
      "|2023-01-02|2:35:00|u1    |2023-01-02 2:35:00|2023-01-02 2:05:00|INTERVAL '0 00:30:00' DAY TO SECOND|\n",
      "|2023-01-02|3:05:00|u1    |2023-01-02 3:05:00|2023-01-02 2:35:00|INTERVAL '0 00:30:00' DAY TO SECOND|\n",
      "|2023-01-12|1:05:00|u2    |2023-01-12 1:05:00|null              |null                               |\n",
      "|2023-01-12|1:05:00|u2    |2023-01-12 1:05:00|2023-01-12 1:05:00|INTERVAL '0 00:00:00' DAY TO SECOND|\n",
      "|2023-01-12|1:05:00|u2    |2023-01-12 1:05:00|2023-01-12 1:05:00|INTERVAL '0 00:00:00' DAY TO SECOND|\n",
      "|2023-01-12|1:35:00|u2    |2023-01-12 1:35:00|2023-01-12 1:05:00|INTERVAL '0 00:30:00' DAY TO SECOND|\n",
      "+----------+-------+------+------------------+------------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date_formatted.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "4ad56f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+------------------+------------------+--------------------+-----------------+\n",
      "|      date|   time|userid|         datetime_|       lag_session|  session_difference|session_unix_time|\n",
      "+----------+-------+------+------------------+------------------+--------------------+-----------------+\n",
      "|2023-01-01|1:05:00|    u1|2023-01-01 1:05:00|              null|                null|             null|\n",
      "|2023-01-01|1:05:00|    u1|2023-01-01 1:05:00|2023-01-01 1:05:00|INTERVAL '0 00:00...|                0|\n",
      "|2023-01-01|1:25:00|    u1|2023-01-01 1:25:00|2023-01-01 1:05:00|INTERVAL '0 00:20...|             1200|\n",
      "|2023-01-02|2:05:00|    u1|2023-01-02 2:05:00|2023-01-01 1:25:00|INTERVAL '1 00:40...|            88800|\n",
      "|2023-01-02|2:35:00|    u1|2023-01-02 2:35:00|2023-01-02 2:05:00|INTERVAL '0 00:30...|             1800|\n",
      "|2023-01-02|3:05:00|    u1|2023-01-02 3:05:00|2023-01-02 2:35:00|INTERVAL '0 00:30...|             1800|\n",
      "|2023-01-12|1:05:00|    u2|2023-01-12 1:05:00|              null|                null|             null|\n",
      "|2023-01-12|1:05:00|    u2|2023-01-12 1:05:00|2023-01-12 1:05:00|INTERVAL '0 00:00...|                0|\n",
      "|2023-01-12|1:05:00|    u2|2023-01-12 1:05:00|2023-01-12 1:05:00|INTERVAL '0 00:00...|                0|\n",
      "|2023-01-12|1:35:00|    u2|2023-01-12 1:35:00|2023-01-12 1:05:00|INTERVAL '0 00:30...|             1800|\n",
      "+----------+-------+------+------------------+------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date_formatted=df_date_formatted.withColumn(\"session_unix_time\",\n",
    "                             F.unix_timestamp(F.to_timestamp(\"datetime_\"))\n",
    "                            )\n",
    "df_date_formatted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "5eaf5211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+------------------+------------------+--------------------+-----------------+-------------------+\n",
      "|      date|   time|userid|         datetime_|       lag_session|  session_difference|session_unix_time| datetime_timestamp|\n",
      "+----------+-------+------+------------------+------------------+--------------------+-----------------+-------------------+\n",
      "|2023-01-01|1:05:00|    u1|2023-01-01 1:05:00|              null|                null|             null|2023-01-01 01:05:00|\n",
      "|2023-01-01|1:05:00|    u1|2023-01-01 1:05:00|2023-01-01 1:05:00|INTERVAL '0 00:00...|                0|2023-01-01 01:05:00|\n",
      "|2023-01-01|1:25:00|    u1|2023-01-01 1:25:00|2023-01-01 1:05:00|INTERVAL '0 00:20...|             1200|2023-01-01 01:25:00|\n",
      "|2023-01-02|2:05:00|    u1|2023-01-02 2:05:00|2023-01-01 1:25:00|INTERVAL '1 00:40...|            88800|2023-01-02 02:05:00|\n",
      "|2023-01-02|2:35:00|    u1|2023-01-02 2:35:00|2023-01-02 2:05:00|INTERVAL '0 00:30...|             1800|2023-01-02 02:35:00|\n",
      "|2023-01-02|3:05:00|    u1|2023-01-02 3:05:00|2023-01-02 2:35:00|INTERVAL '0 00:30...|             1800|2023-01-02 03:05:00|\n",
      "|2023-01-12|1:05:00|    u2|2023-01-12 1:05:00|              null|                null|             null|2023-01-12 01:05:00|\n",
      "|2023-01-12|1:05:00|    u2|2023-01-12 1:05:00|2023-01-12 1:05:00|INTERVAL '0 00:00...|                0|2023-01-12 01:05:00|\n",
      "|2023-01-12|1:05:00|    u2|2023-01-12 1:05:00|2023-01-12 1:05:00|INTERVAL '0 00:00...|                0|2023-01-12 01:05:00|\n",
      "|2023-01-12|1:35:00|    u2|2023-01-12 1:35:00|2023-01-12 1:05:00|INTERVAL '0 00:30...|             1800|2023-01-12 01:35:00|\n",
      "+----------+-------+------+------------------+------------------+--------------------+-----------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- userid: string (nullable = true)\n",
      " |-- datetime_: string (nullable = false)\n",
      " |-- lag_session: string (nullable = true)\n",
      " |-- session_difference: interval day to second (nullable = true)\n",
      " |-- session_unix_time: long (nullable = true)\n",
      " |-- datetime_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date_formatted=df_date_formatted.withColumn(\"datetime_timestamp\",df_date_formatted.datetime_.cast(\"timestamp\"))\n",
    "df_date_formatted.show()\n",
    "df_date_formatted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "b3f0b872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|month|\n",
      "+-----+\n",
      "|    1|\n",
      "|    1|\n",
      "|    2|\n",
      "|    2|\n",
      "|    2|\n",
      "|   12|\n",
      "|   12|\n",
      "|   12|\n",
      "|    1|\n",
      "|   12|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_date_formatted.withColumn(\"month\",F.expr(\"extract(month from 'datetime_timestamp')\")).show()\n",
    "df_date_formatted.withColumn(\"day\",F.dayofmonth(\"datetime_timestamp\")).select(\"day\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "47dc1f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+------------------+------------------+--------------------+-----------------+-------------------+---+-----+----+----+------+------+\n",
      "|      date|   time|userid|         datetime_|       lag_session|  session_difference|session_unix_time| datetime_timestamp|day|month|year|hour|minute|second|\n",
      "+----------+-------+------+------------------+------------------+--------------------+-----------------+-------------------+---+-----+----+----+------+------+\n",
      "|2023-01-01|1:05:00|    u1|2023-01-01 1:05:00|              null|                null|             null|2023-01-01 01:05:00|  1|    1|2023|  01|    05|    00|\n",
      "|2023-01-01|1:05:00|    u1|2023-01-01 1:05:00|2023-01-01 1:05:00|INTERVAL '0 00:00...|                0|2023-01-01 01:05:00|  1|    1|2023|  01|    05|    00|\n",
      "|2023-01-01|1:25:00|    u1|2023-01-01 1:25:00|2023-01-01 1:05:00|INTERVAL '0 00:20...|             1200|2023-01-01 01:25:00|  1|    1|2023|  01|    25|    00|\n",
      "|2023-01-02|2:05:00|    u1|2023-01-02 2:05:00|2023-01-01 1:25:00|INTERVAL '1 00:40...|            88800|2023-01-02 02:05:00|  2|    1|2023|  02|    05|    00|\n",
      "|2023-01-02|2:35:00|    u1|2023-01-02 2:35:00|2023-01-02 2:05:00|INTERVAL '0 00:30...|             1800|2023-01-02 02:35:00|  2|    1|2023|  02|    35|    00|\n",
      "|2023-01-02|3:05:00|    u1|2023-01-02 3:05:00|2023-01-02 2:35:00|INTERVAL '0 00:30...|             1800|2023-01-02 03:05:00|  2|    1|2023|  03|    05|    00|\n",
      "|2023-01-12|1:05:00|    u2|2023-01-12 1:05:00|              null|                null|             null|2023-01-12 01:05:00| 12|    1|2023|  01|    05|    00|\n",
      "|2023-01-12|1:05:00|    u2|2023-01-12 1:05:00|2023-01-12 1:05:00|INTERVAL '0 00:00...|                0|2023-01-12 01:05:00| 12|    1|2023|  01|    05|    00|\n",
      "|2023-01-12|1:05:00|    u2|2023-01-12 1:05:00|2023-01-12 1:05:00|INTERVAL '0 00:00...|                0|2023-01-12 01:05:00| 12|    1|2023|  01|    05|    00|\n",
      "|2023-01-12|1:35:00|    u2|2023-01-12 1:35:00|2023-01-12 1:05:00|INTERVAL '0 00:30...|             1800|2023-01-12 01:35:00| 12|    1|2023|  01|    35|    00|\n",
      "+----------+-------+------+------------------+------------------+--------------------+-----------------+-------------------+---+-----+----+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date_formatted=(df_date_formatted\n",
    " .withColumn(\"day\",F.date_format(\"datetime_timestamp\",\"d\"))\n",
    " .withColumn(\"month\",F.date_format(\"datetime_timestamp\",\"M\"))\n",
    " .withColumn(\"year\",F.date_format(\"datetime_timestamp\",\"y\"))\n",
    " .withColumn(\"hour\",F.date_format(\"datetime_timestamp\",\"hh\"))\n",
    " .withColumn(\"minute\",F.date_format(\"datetime_timestamp\",\"mm\"))\n",
    " .withColumn(\"second\",F.date_format(\"datetime_timestamp\",\"ss\")))\n",
    "#.select(\"day\",\"month\",\"year\",\"hour\",\"minute\",\"second\")\n",
    "df_date_formatted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "bd2a8733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-----------------+----+-----+----+------+------+\n",
      "|userid| datetime_timestamp|session_unix_time|year|month|hour|minute|second|\n",
      "+------+-------------------+-----------------+----+-----+----+------+------+\n",
      "|    u1|2023-01-01 01:05:00|             null|2023|    1|  01|    05|    00|\n",
      "|    u1|2023-01-01 01:05:00|                0|2023|    1|  01|    05|    00|\n",
      "|    u1|2023-01-01 01:25:00|             1200|2023|    1|  01|    25|    00|\n",
      "|    u1|2023-01-02 02:05:00|            88800|2023|    1|  02|    05|    00|\n",
      "|    u1|2023-01-02 02:35:00|             1800|2023|    1|  02|    35|    00|\n",
      "|    u1|2023-01-02 03:05:00|             1800|2023|    1|  03|    05|    00|\n",
      "|    u2|2023-01-12 01:05:00|             null|2023|    1|  01|    05|    00|\n",
      "|    u2|2023-01-12 01:05:00|                0|2023|    1|  01|    05|    00|\n",
      "|    u2|2023-01-12 01:05:00|                0|2023|    1|  01|    05|    00|\n",
      "|    u2|2023-01-12 01:35:00|             1800|2023|    1|  01|    35|    00|\n",
      "+------+-------------------+-----------------+----+-----+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_selected=df_date_formatted.select(\"userid\",\"datetime_timestamp\",\"session_unix_time\",\"year\",\"month\",\"hour\",\"minute\",\"second\")\n",
    "df_selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "8736a445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-----------------+----+-----+----+------+------+----------------+--------------+\n",
      "|userid| datetime_timestamp|session_unix_time|year|month|hour|minute|second|timediff_minutes|timediff_hours|\n",
      "+------+-------------------+-----------------+----+-----+----+------+------+----------------+--------------+\n",
      "|    u1|2023-01-01 01:05:00|             null|2023|    1|  01|    05|    00|            null|          null|\n",
      "|    u1|2023-01-01 01:05:00|                0|2023|    1|  01|    05|    00|             0.0|           0.0|\n",
      "|    u1|2023-01-01 01:25:00|             1200|2023|    1|  01|    25|    00|            20.0|           0.0|\n",
      "|    u1|2023-01-02 02:05:00|            88800|2023|    1|  02|    05|    00|          1480.0|          25.0|\n",
      "|    u1|2023-01-02 02:35:00|             1800|2023|    1|  02|    35|    00|            30.0|           1.0|\n",
      "|    u1|2023-01-02 03:05:00|             1800|2023|    1|  03|    05|    00|            30.0|           1.0|\n",
      "|    u2|2023-01-12 01:05:00|             null|2023|    1|  01|    05|    00|            null|          null|\n",
      "|    u2|2023-01-12 01:05:00|                0|2023|    1|  01|    05|    00|             0.0|           0.0|\n",
      "|    u2|2023-01-12 01:05:00|                0|2023|    1|  01|    05|    00|             0.0|           0.0|\n",
      "|    u2|2023-01-12 01:35:00|             1800|2023|    1|  01|    35|    00|            30.0|           1.0|\n",
      "+------+-------------------+-----------------+----+-----+----+------+------+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_selected=(df_selected.withColumn(\"timediff_minutes\",F.col(\"session_unix_time\")/60)\n",
    " .withColumn(\"timediff_hours\",F.round(F.col(\"session_unix_time\")/(60*60))))\n",
    "df_selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c05aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ce95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "79d49582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('date', 'string'), ('time', 'string'), ('userid', 'string')]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_date_formatted.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "723fb581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'datetime': '2023-01-01 1:05:00', 'userid': 'u1'},\n",
       " {'datetime': '2023-01-01 2:05:00', 'userid': 'u1'},\n",
       " {'datetime': '2023-01-12 1:05:00', 'userid': 'u2'},\n",
       " {'datetime': '2023-01-01 1:05:00', 'userid': 'u1'},\n",
       " {'datetime': '2023-01-12 1:35:00', 'userid': 'u2'}]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe={\"datetime\":None,\"userid\":None}\n",
    "df_1=[]\n",
    "for item in val:\n",
    "    dataframe[\"datetime\"]=item[0]+\" \"+item[1]\n",
    "    dataframe[\"userid\"]=item[-1]\n",
    "    df_1.append(dataframe)\n",
    "    dataframe={}\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7d3e5a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---+---+---+---+---+---+\n",
      "|        _1|     _2| _3| _4| _5| _6| _7| _8|\n",
      "+----------+-------+---+---+---+---+---+---+\n",
      "|2023-01-01|1:05:00|   |   |   |   |   | u1|\n",
      "|2023-01-01|1:25:00|   |   |   |   |   | u1|\n",
      "|2023-01-02|2:05:00|   |   |   |   |   | u1|\n",
      "|2023-01-02|2:35:00|   |   |   |   |   | u1|\n",
      "|2023-01-02|3:05:00|   |   |   |   |   | u1|\n",
      "|2023-01-12|1:05:00|   |   |   |   |   | u2|\n",
      "|2023-01-12|1:05:00|   |   |   |   |   | u2|\n",
      "|2023-01-12|1:05:00|   |   |   |   |   | u2|\n",
      "|2023-01-01|1:05:00|   |   |   |   |   | u1|\n",
      "|2023-01-12|1:35:00|   |   |   |   |   | u2|\n",
      "+----------+-------+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1b466f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema =\"\"\"\n",
    "timestamp timestamp,\n",
    "userid string\n",
    "\"\"\"\n",
    "arr=[]\n",
    "with open(filepath,\"r\") as f:\n",
    "    arr.append(f.readlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5d7247bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023-01-01 1:05:00  u1\\n',\n",
       " '2023-01-01 2:05:00  u1\\n',\n",
       " '2023-01-12 1:05:00  u2\\n',\n",
       " '2023-01-01 1:05:00  u1\\n',\n",
       " '2023-01-12 1:35:00  u2']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_dataframe=arr[0][7:]\n",
    "array_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2d48abb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023-01-01', '1:05:00', '', 'u1\\n']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_dataframe[0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba39e4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
