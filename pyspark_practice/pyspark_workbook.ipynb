{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10f5d2fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "JAR_LOCATION=r\"C:\\Users\\abhishek\\Documents\\coding\\ds-algo\\pyspark_practice\\postgresql-42.5.2.jar\"\n",
    "URL = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "PROPERTIES = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"123\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "spark = (SparkSession.builder.master(\"local[*]\")\n",
    "        .config(\"spark.driver.maxResultSize\",  \"0\")\n",
    "        .config(\"spark.executor.memory\", \"4g\")\n",
    "         #.config(\"spark.jars\", jar_location) # never use this\n",
    "        .config(\"spark.driver.extraClassPath\", JAR_LOCATION)\n",
    "        .config(\"spark.driver.maxResultSize\",  \"0\")\n",
    "        .getOrCreate())\n",
    "#spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n",
    "\n",
    "def create_rdd(spark):\n",
    "    employee = [(\"Radhika\",10), \n",
    "        (\"Kaavya\",20), \n",
    "        (\"Varnika\",30), \n",
    "        (\"Akshay\",40) \n",
    "      ]\n",
    "    rdd = spark.sparkContext.parallelize(employee)\n",
    "    dataframee = rdd.toDF()\n",
    "    dataframee.printSchema()\n",
    "    dataframee.show()\n",
    "def read_csv(spark,filename):\n",
    "    return spark.read.csv(filename)\n",
    "\n",
    "def read_postgres_connection(spark,table_name):\n",
    "    df = spark.read.jdbc(URL, table_name, properties=PROPERTIES)\n",
    "    return df\n",
    "#df=read_postgres_connection(spark,table_name=\"public.employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2de85c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import expr, dense_rank,col\n",
    "def get_salary():\n",
    "    w=Window().orderBy(\"salary\").partitionBy(\"department\")\n",
    "    df_salary_rank=df.withColumn(\"salary_rank\",dense_rank().over(w))\n",
    "    df_temp=df_salary_rank.select([\"id\",\"salary\",\"salary_rank\"])\n",
    "    df_temp1=df_temp.filter(col(\"salary_rank\")==3)\n",
    "    df_temp1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef18fd06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----------+\n",
      "|department|sex|summ_salary|\n",
      "+----------+---+-----------+\n",
      "|     Sales|  M|      17800|\n",
      "|     Audit|  M|       2100|\n",
      "|     Audit|  F|       1700|\n",
      "|Management|  F|     350000|\n",
      "|     Sales|  F|      11600|\n",
      "|Management|  M|     350000|\n",
      "+----------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql.functions import sum\n",
    "\n",
    "#df.groupby(\"department\",\"sex\").sum(\"salary\").withColumnRenamed(\"sum(salary)\",\"summ_salary\").show()\n",
    "\n",
    "#agg(sum(\"salary\").alias(\"sum_salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "322c50c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word=\"abcdefa\"\n",
    "spark_df=spark.sparkContext.textFile(r\"C:\\Users\\abhishek\\Documents\\coding\\ds-algo\\interview_test\\word_count.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d908b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc def',\n",
       " 'abc def',\n",
       " 'abc def',\n",
       " 'abc def',\n",
       " 'abc def',\n",
       " 'abc def',\n",
       " 'abc def',\n",
       " 'abc def']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fd20794",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc',\n",
       " 'def',\n",
       " 'abc',\n",
       " 'def',\n",
       " 'abc',\n",
       " 'def',\n",
       " 'abc',\n",
       " 'def',\n",
       " 'abc',\n",
       " 'def',\n",
       " 'abc',\n",
       " 'def',\n",
       " 'abc',\n",
       " 'def',\n",
       " 'abc',\n",
       " 'def']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.flatMap(lambda x:x.split(\" \")).collect()\n",
    "#map(lambda : word,word_dict.get(word,1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24b36f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abc', 1),\n",
       " ('def', 1),\n",
       " ('abc', 1),\n",
       " ('def', 1),\n",
       " ('abc', 1),\n",
       " ('def', 1),\n",
       " ('abc', 1),\n",
       " ('def', 1),\n",
       " ('abc', 1),\n",
       " ('def', 1),\n",
       " ('abc', 1),\n",
       " ('def', 1),\n",
       " ('abc', 1),\n",
       " ('def', 1),\n",
       " ('abc', 1),\n",
       " ('def', 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.flatMap(lambda x:x.split(\" \")).map(lambda y:(y,1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53af1a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abc', 8), ('def', 8)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.flatMap(lambda x:x.split(\" \")).map(lambda y:(y,1)).reduceByKey(lambda a,b: a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51eefc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e', 'f', 'a']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word=\"abcdefa\"\n",
    "df=spark.sparkContext.parallelize(word)\n",
    "df.collect()\n",
    "df.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0b004d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d', 1), ('e', 1), ('b', 1), ('c', 1), ('a', 2), ('f', 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff443cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "536d41c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns#.select(\"order_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0443d5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "223698ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+----------------+-------------------+-------------------+\n",
      "|cust_id|         order_date|order_details|total_order_cost|          next_date|     next_next_date|\n",
      "+-------+-------------------+-------------+----------------+-------------------+-------------------+\n",
      "|      7|2019-04-03 00:00:00|      Dresses|              50|2019-04-02 00:00:00|2019-04-01 00:00:00|\n",
      "|      7|2019-04-04 00:00:00|         Coat|              25|2019-04-03 00:00:00|2019-04-02 00:00:00|\n",
      "|      7|2019-04-19 00:00:00|         Suit|             150|2019-04-04 00:00:00|2019-04-03 00:00:00|\n",
      "|     12|2019-03-11 00:00:00|      Slipper|              20|2019-01-11 00:00:00|2019-01-11 00:00:00|\n",
      "+-------+-------------------+-------------+----------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query=\"\"\"\n",
    "select *\n",
    "from \n",
    "(\n",
    "select *,\n",
    "lag(order_date,1) over(partition by cust_id order by order_date asc) as next_date,\n",
    "lag(order_date,2) over(partition by cust_id order by order_date asc) as next_next_date\n",
    "from orders\n",
    ")\n",
    "where datediff(next_date,next_next_date)<2\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6906896",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=spark.sparkContext.parallelize(range(6,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85a493ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6033921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90b9cd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+----------------+\n",
      "|cust_id|         order_date|order_details|total_order_cost|\n",
      "+-------+-------------------+-------------+----------------+\n",
      "|      3|2019-03-04 00:00:00|         Coat|             100|\n",
      "|      3|2019-03-01 00:00:00|        Shoes|              80|\n",
      "|      3|2019-03-07 00:00:00|        Skirt|              30|\n",
      "|      7|2019-02-01 00:00:00|         Coat|              25|\n",
      "|      7|2019-03-10 00:00:00|        Shoes|              80|\n",
      "|     15|2019-02-01 00:00:00|        Boats|             100|\n",
      "|     15|2019-01-11 00:00:00|       Shirts|              60|\n",
      "|     15|2019-03-11 00:00:00|      Slipper|              20|\n",
      "|     15|2019-03-01 00:00:00|        Jeans|              80|\n",
      "|     15|2019-03-09 00:00:00|       Shirts|              50|\n",
      "|      5|2019-02-01 00:00:00|        Shoes|              80|\n",
      "|     12|2019-01-11 00:00:00|       Shirts|              60|\n",
      "|     12|2019-03-11 00:00:00|      Slipper|              20|\n",
      "|      4|2019-02-01 00:00:00|        Shoes|              80|\n",
      "|      4|2019-01-11 00:00:00|       Shirts|              60|\n",
      "|      3|2019-04-19 00:00:00|       Shirts|              50|\n",
      "|      7|2019-04-19 00:00:00|         Suit|             150|\n",
      "|     15|2019-04-19 00:00:00|        Skirt|              30|\n",
      "|     15|2019-04-20 00:00:00|      Dresses|             200|\n",
      "|     12|2019-01-11 00:00:00|         Coat|             125|\n",
      "+-------+-------------------+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae72de2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+----------------+\n",
      "|cust_id|         order_date|order_details|total_order_cost|\n",
      "+-------+-------------------+-------------+----------------+\n",
      "|      5|2019-02-04 00:00:00|         None|             200|\n",
      "|      5|2019-02-04 00:00:00|       Skirt1|            null|\n",
      "|      5|2019-02-04 00:00:00|         null|            null|\n",
      "+-------+-------------------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=read_postgres_connection(spark,table_name=\"public.orders\")\n",
    "data = [('5', '2019-02-04 00:00:00', \"None\",\"200\"),\n",
    "        ('5', '2019-02-04 00:00:00', \"Skirt1\",'null'),\n",
    "        ('5', '2019-02-04 00:00:00', None,None)]\n",
    "columns = [\"cust_id\", \"order_date\", \"order_details\",\"total_order_cost\"]\n",
    "df_1 = spark.createDataFrame(data, columns)\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea0468bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+----------------+\n",
      "|cust_id|         order_date|order_details|total_order_cost|\n",
      "+-------+-------------------+-------------+----------------+\n",
      "|      3|2019-03-04 00:00:00|         Coat|             100|\n",
      "|      3|2019-03-01 00:00:00|        Shoes|              80|\n",
      "|      3|2019-03-07 00:00:00|        Skirt|              30|\n",
      "|      7|2019-02-01 00:00:00|         Coat|              25|\n",
      "|      7|2019-03-10 00:00:00|        Shoes|              80|\n",
      "|     15|2019-02-01 00:00:00|        Boats|             100|\n",
      "|     15|2019-01-11 00:00:00|       Shirts|              60|\n",
      "|     15|2019-03-11 00:00:00|      Slipper|              20|\n",
      "|     15|2019-03-01 00:00:00|        Jeans|              80|\n",
      "|     15|2019-03-09 00:00:00|       Shirts|              50|\n",
      "|      5|2019-02-01 00:00:00|        Shoes|              80|\n",
      "|     12|2019-01-11 00:00:00|       Shirts|              60|\n",
      "|     12|2019-03-11 00:00:00|      Slipper|              20|\n",
      "|      4|2019-02-01 00:00:00|        Shoes|              80|\n",
      "|      4|2019-01-11 00:00:00|       Shirts|              60|\n",
      "|      3|2019-04-19 00:00:00|       Shirts|              50|\n",
      "|      7|2019-04-19 00:00:00|         Suit|             150|\n",
      "|     15|2019-04-19 00:00:00|        Skirt|              30|\n",
      "|     15|2019-04-20 00:00:00|      Dresses|             200|\n",
      "|     12|2019-01-11 00:00:00|         Coat|             125|\n",
      "+-------+-------------------+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders=df.union(df_1)\n",
    "df_orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86e47080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+----------------+\n",
      "|cust_id|order_details|total_order_cost|\n",
      "+-------+-------------+----------------+\n",
      "|      0|            2|               2|\n",
      "+-------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    [count(\n",
    "        when(isnan(c) \n",
    "             | col(c).isNull() \n",
    "             | col(c).contains('null')\n",
    "             |col(c).contains('None')\n",
    "             , c)\n",
    "    ).alias(c) for c in df.columns if c !='order_date'\n",
    "    ]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "835d6921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+----------------+\n",
      "|cust_id|         order_date|order_details|total_order_cost|\n",
      "+-------+-------------------+-------------+----------------+\n",
      "|      5|2019-02-04 00:00:00|         None|             200|\n",
      "+-------+-------------------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"order_details\").contains(\"None\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e85b021a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+----------------+\n",
      "|cust_id|         order_date|order_details|total_order_cost|\n",
      "+-------+-------------------+-------------+----------------+\n",
      "|      5|2019-02-04 00:00:00|         None|             200|\n",
      "|      5|2019-02-04 00:00:00|         null|            null|\n",
      "+-------+-------------------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((isnan(\"order_details\"))|(col(\"order_details\").isNull())|(col(\"order_details\").contains(\"None\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12ee006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count((cust_id IS NULL))|\n",
      "+------------------------+\n",
      "|                      25|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(count(col('cust_id').isNull())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a5761e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-------------+----------------+\n",
      "|cust_id|order_date|order_details|total_order_cost|\n",
      "+-------+----------+-------------+----------------+\n",
      "+-------+----------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col('cust_id').isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ba38d3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------+----------------+-----+\n",
      "|cust_id|         order_date|order_details|total_order_cost|index|\n",
      "+-------+-------------------+-------------+----------------+-----+\n",
      "|      3|2019-03-04 00:00:00|         Coat|             100|    0|\n",
      "|      3|2019-03-01 00:00:00|        Shoes|              80|    1|\n",
      "|      3|2019-03-07 00:00:00|        Skirt|              30|    2|\n",
      "|      7|2019-02-01 00:00:00|         Coat|              25|    3|\n",
      "|      7|2019-03-10 00:00:00|        Shoes|              80|    4|\n",
      "|     15|2019-02-01 00:00:00|        Boats|             100|    5|\n",
      "|     15|2019-01-11 00:00:00|       Shirts|              60|    6|\n",
      "|     15|2019-03-11 00:00:00|      Slipper|              20|    7|\n",
      "|     15|2019-03-01 00:00:00|        Jeans|              80|    8|\n",
      "|     15|2019-03-09 00:00:00|       Shirts|              50|    9|\n",
      "|      5|2019-02-01 00:00:00|        Shoes|              80|   10|\n",
      "|     12|2019-01-11 00:00:00|       Shirts|              60|   11|\n",
      "|     12|2019-03-11 00:00:00|      Slipper|              20|   12|\n",
      "|      4|2019-02-01 00:00:00|        Shoes|              80|   13|\n",
      "|      4|2019-01-11 00:00:00|       Shirts|              60|   14|\n",
      "|      3|2019-04-19 00:00:00|       Shirts|              50|   15|\n",
      "|      7|2019-04-19 00:00:00|         Suit|             150|   16|\n",
      "|     15|2019-04-19 00:00:00|        Skirt|              30|   17|\n",
      "|     15|2019-04-20 00:00:00|      Dresses|             200|   18|\n",
      "|     12|2019-01-11 00:00:00|         Coat|             125|   19|\n",
      "+-------+-------------------+-------------+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "df=df.withColumn(\"index\",row_number().over(w)-1)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "659cd087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| c1| c2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  b|  2|\n",
      "|  c|  3|\n",
      "|  d|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list1 = [\"a\", \"b\", \"c\", \"d\"]\n",
    "list2 = [1, 2, 3, 4]\n",
    "rdd1=spark.sparkContext.parallelize(list(zip(list1,list2)))\n",
    "rdd1_df=rdd1.toDF([\"c1\",\"c2\"])\n",
    "rdd1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "612d540b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| c1|\n",
      "+---+\n",
      "|  a|\n",
      "|  b|\n",
      "|  c|\n",
      "|  d|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd1_df1=rdd1_df.select(\"c1\")\n",
    "rdd1_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e8aecf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rdd1_df2=spark.sparkContext.parallelize([\"a\",\"b\"])\n",
    "#rdd1_df2.toDF([\"c2\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd9f40ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "list_A = [1, 2, 3, 4, 5]\n",
    "list_B = [4, 5, 6, 7, 8]\n",
    "sc=spark.sparkContext\n",
    "rdd_A = sc.parallelize(list_A)\n",
    "rdd_B = sc.parallelize(list_B)\n",
    "\n",
    "# Perform subtract operation\n",
    "result_rdd = rdd_A.subtract(rdd_B)\n",
    "\n",
    "# Collect result\n",
    "result_list = result_rdd.collect()\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16699647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   A| 10|\n",
      "|   B| 20|\n",
      "|   C| 30|\n",
      "|   D| 40|\n",
      "|   E| 50|\n",
      "|   F| 15|\n",
      "|   G| 28|\n",
      "|   H| 54|\n",
      "|   I| 41|\n",
      "|   J| 86|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef0a1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.0, 20.0, 30.0, 50.0, 86.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles=df.approxQuantile(\"Age\",[0.0,0.25,0.5,0.75,1.0],0.01)\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa942cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               Age|\n",
      "+-------+------------------+\n",
      "|  count|                10|\n",
      "|   mean|              37.4|\n",
      "| stddev|22.396428286671068|\n",
      "|    min|                10|\n",
      "|    max|                86|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe([\"Age\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44695b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   B| 20|\n",
      "|   A| 10|\n",
      "|   C| 30|\n",
      "|   D| 40|\n",
      "|   E| 50|\n",
      "|   F| 15|\n",
      "|   H| 54|\n",
      "|   I| 41|\n",
      "|   G| 28|\n",
      "|   J| 86|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.distinct().show()#.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2790b911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|Age|\n",
      "+---+\n",
      "| 10|\n",
      "| 20|\n",
      "| 30|\n",
      "| 40|\n",
      "| 50|\n",
      "| 15|\n",
      "| 28|\n",
      "| 54|\n",
      "| 41|\n",
      "| 86|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "494d05d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- knownLanguages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n",
      "+----------+-------------------+--------------------+\n",
      "|      name|     knownLanguages|          properties|\n",
      "+----------+-------------------+--------------------+\n",
      "|     James|      [Java, Scala]|{eye -> brown, ha...|\n",
      "|   Michael|[Spark, Java, null]|{eye -> null, hai...|\n",
      "|    Robert|         [CSharp, ]|{eye -> , hair ->...|\n",
      "|Washington|               null|                null|\n",
      "| Jefferson|             [1, 2]|                  {}|\n",
      "+----------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pyspark-by-examples').getOrCreate()\n",
    "\n",
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['name','knownLanguages','properties'])\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abb3dc2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- col: string (nullable = true)\n",
      "\n",
      "+---------+------+\n",
      "|     name|   col|\n",
      "+---------+------+\n",
      "|    James|  Java|\n",
      "|    James| Scala|\n",
      "|  Michael| Spark|\n",
      "|  Michael|  Java|\n",
      "|  Michael|  null|\n",
      "|   Robert|CSharp|\n",
      "|   Robert|      |\n",
      "|Jefferson|     1|\n",
      "|Jefferson|     2|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df2 = df.select(df.name,explode(df.knownLanguages))\n",
    "df2.printSchema()\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a0b15f-620c-469e-8c3e-c46d53c4a4cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- key: string (nullable = false)\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "+-------+----+-----+\n",
      "|   name| key|value|\n",
      "+-------+----+-----+\n",
      "|  James| eye|brown|\n",
      "|  James|hair|black|\n",
      "|Michael| eye| null|\n",
      "|Michael|hair|brown|\n",
      "| Robert| eye|     |\n",
      "| Robert|hair|  red|\n",
      "+-------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import explode\n",
    "df3 = df.select(df.name,explode(df.properties))\n",
    "df3.printSchema()\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69dfec8b-ac1e-419d-9400-96a32fe78d18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      name|   col|\n",
      "+----------+------+\n",
      "|     James|  Java|\n",
      "|     James| Scala|\n",
      "|   Michael| Spark|\n",
      "|   Michael|  Java|\n",
      "|   Michael|  null|\n",
      "|    Robert|CSharp|\n",
      "|    Robert|      |\n",
      "|Washington|  null|\n",
      "| Jefferson|     1|\n",
      "| Jefferson|     2|\n",
      "+----------+------+\n",
      "\n",
      "+----------+----+-----+\n",
      "|      name| key|value|\n",
      "+----------+----+-----+\n",
      "|     James| eye|brown|\n",
      "|     James|hair|black|\n",
      "|   Michael| eye| null|\n",
      "|   Michael|hair|brown|\n",
      "|    Robert| eye|     |\n",
      "|    Robert|hair|  red|\n",
      "|Washington|null| null|\n",
      "| Jefferson|null| null|\n",
      "+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import explode_outer\n",
    "\"\"\" with array \"\"\"\n",
    "df.select(df.name,explode_outer(df.knownLanguages)).show()\n",
    "\"\"\" with map \"\"\"\n",
    "df.select(df.name,explode_outer(df.properties)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd1aee39-2699-4bc9-ba8d-e10f49f77559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|deviceId|  segment|\n",
      "+--------+---------+\n",
      "|       1|[a, b, c]|\n",
      "|       2|   [b, d]|\n",
      "|       3|[e, a, c]|\n",
      "|       4|[o, d, e]|\n",
      "|       5|      [p]|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05259b4b-13fe-4cc1-b1b0-0c08227d6013",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|deviceId|col|\n",
      "+--------+---+\n",
      "|       1|  a|\n",
      "|       1|  b|\n",
      "|       1|  c|\n",
      "|       2|  b|\n",
      "|       2|  d|\n",
      "|       3|  e|\n",
      "|       3|  a|\n",
      "|       3|  c|\n",
      "|       4|  o|\n",
      "|       4|  d|\n",
      "|       4|  e|\n",
      "|       5|  p|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b7d92e2-b350-4b61-9a93-0339d018abdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|deviceId|  segment|\n",
      "+--------+---------+\n",
      "|       1|[a, b, c]|\n",
      "|       2|   [b, d]|\n",
      "|       3|[e, a, c]|\n",
      "|       4|[o, d, e]|\n",
      "|       5|      [p]|\n",
      "+--------+---------+\n",
      "\n",
      "+--------+---+\n",
      "|deviceId|col|\n",
      "+--------+---+\n",
      "|       1|  a|\n",
      "|       1|  b|\n",
      "|       1|  c|\n",
      "|       2|  b|\n",
      "|       2|  d|\n",
      "|       3|  e|\n",
      "|       3|  a|\n",
      "|       3|  c|\n",
      "|       4|  o|\n",
      "|       4|  d|\n",
      "|       4|  e|\n",
      "|       5|  p|\n",
      "+--------+---+\n",
      "\n",
      "+---+----------------------+\n",
      "|col|collect_list(deviceId)|\n",
      "+---+----------------------+\n",
      "|  c|                [1, 3]|\n",
      "|  b|                [1, 2]|\n",
      "|  a|                [1, 3]|\n",
      "|  d|                [2, 4]|\n",
      "|  e|                [3, 4]|\n",
      "|  o|                   [4]|\n",
      "|  p|                   [5]|\n",
      "+---+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arrayData = [\n",
    "        (1,['a','b','c']),\n",
    "        (2,['b','d']),\n",
    "        (3,['e','a','c']),\n",
    "        (4,[\"o\",\"d\",\"e\"]),\n",
    "        (5,['p'])]\n",
    "\n",
    "df = spark.createDataFrame(data=arrayData, schema = ['deviceId','segment'])\n",
    "df.show()\n",
    "\n",
    "df1=df.select(df.deviceId,F.explode(df.segment))\n",
    "df1.show()\n",
    "df2=df1.groupBy(\"col\").agg(F.collect_list(\"deviceId\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aaeece-d936-4201-91ee-87bd9a9094ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e56059-82a3-4d6e-ad2d-6ca702e35210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d4406e-4ff7-45ea-9b30-5a603c9cfe45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
